{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#installing packages\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy.sparse as sparse\n",
    "import operator\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the LogisticMF file\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class LogisticMF:\n",
    "\n",
    "    def __init__(self, counts, num_factors=40, reg_param=100.0, gamma=1.0,\n",
    "                 iterations=40, num_users = 6518, num_items = 4036):\n",
    "        self.counts = counts\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_factors = num_factors\n",
    "        self.iterations = iterations\n",
    "        self.reg_param = reg_param\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def train_model(self):\n",
    "\n",
    "        self.ones = np.ones((self.num_users, self.num_items))\n",
    "        self.user_vectors = np.random.normal(size=(self.num_users,\n",
    "                                                   self.num_factors))\n",
    "        self.item_vectors = np.random.normal(size=(self.num_items,\n",
    "                                                   self.num_factors))\n",
    "        self.user_biases = np.random.normal(size=(self.num_users, 1))\n",
    "        self.item_biases = np.random.normal(size=(self.num_items, 1))\n",
    "\n",
    "        user_vec_deriv_sum = np.zeros((self.num_users, self.num_factors))\n",
    "        item_vec_deriv_sum = np.zeros((self.num_items, self.num_factors))\n",
    "        user_bias_deriv_sum = np.zeros((self.num_users, 1))\n",
    "        item_bias_deriv_sum = np.zeros((self.num_items, 1))\n",
    "        for i in range(self.iterations):\n",
    "            # Fix items and solve for users\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            user_vec_deriv, user_bias_deriv = self.deriv(True)\n",
    "            user_vec_deriv_sum += np.square(user_vec_deriv)\n",
    "            user_bias_deriv_sum += np.square(user_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(user_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(user_bias_deriv_sum)\n",
    "            self.user_vectors += vec_step_size * user_vec_deriv\n",
    "            self.user_biases += bias_step_size * user_bias_deriv\n",
    "\n",
    "            # Fix users and solve for items\n",
    "            # take step towards gradient of deriv of log likelihood\n",
    "            # we take a step in positive direction because we are maximizing LL\n",
    "            item_vec_deriv, item_bias_deriv = self.deriv(False)\n",
    "            item_vec_deriv_sum += np.square(item_vec_deriv)\n",
    "            item_bias_deriv_sum += np.square(item_bias_deriv)\n",
    "            vec_step_size = self.gamma / np.sqrt(item_vec_deriv_sum)\n",
    "            bias_step_size = self.gamma / np.sqrt(item_bias_deriv_sum)\n",
    "            self.item_vectors += vec_step_size * item_vec_deriv\n",
    "            self.item_biases += bias_step_size * item_bias_deriv\n",
    "\n",
    "    def deriv(self, user):\n",
    "        if user:\n",
    "            vec_deriv = np.dot(self.counts, self.item_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=1), 1)\n",
    "\n",
    "        else:\n",
    "            vec_deriv = np.dot(self.counts.T, self.user_vectors)\n",
    "            bias_deriv = np.expand_dims(np.sum(self.counts, axis=0), 1)\n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        A = np.exp(A)\n",
    "        A /= (A + self.ones)\n",
    "        A = (self.counts + self.ones) * A\n",
    "\n",
    "        if user:\n",
    "            vec_deriv -= np.dot(A, self.item_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=1), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.user_vectors\n",
    "        else:\n",
    "            vec_deriv -= np.dot(A.T, self.user_vectors)\n",
    "            bias_deriv -= np.expand_dims(np.sum(A, axis=0), 1)\n",
    "            # L2 regularization\n",
    "            vec_deriv -= self.reg_param * self.item_vectors\n",
    "        return (vec_deriv, bias_deriv)\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        loglik = 0\n",
    "        A = np.dot(self.user_vectors, self.item_vectors.T)\n",
    "        A += self.user_biases\n",
    "        A += self.item_biases.T\n",
    "        B = A * self.counts\n",
    "        loglik += np.sum(B)\n",
    "\n",
    "        A = np.exp(A)\n",
    "        A += self.ones\n",
    "\n",
    "        A = np.log(A)\n",
    "        A = (self.counts + self.ones) * A\n",
    "        loglik -= np.sum(A)\n",
    "\n",
    "        # L2 regularization\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.user_vectors))\n",
    "        loglik -= 0.5 * self.reg_param * np.sum(np.square(self.item_vectors))\n",
    "        return loglik\n",
    "\n",
    "    def print_vectors(self):\n",
    "        user_vecs_file = open('logmf-user-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_users):\n",
    "            vec = ','.join(map(str, self.user_vectors[i]))\n",
    "            line = '%i,%s\\n' % (i, vec)\n",
    "            user_vecs_file.write(line)\n",
    "        user_vecs_file.close()\n",
    "        item_vecs_file = open('logmf-item-vecs-%i' % self.num_factors, 'w')\n",
    "        for i in range(self.num_items):\n",
    "            vec = ','.join(map(str, self.item_vectors[i]))\n",
    "            line = '%i,%s\\n' % (i, vec)\n",
    "            item_vecs_file.write(line)\n",
    "        a = item_vecs_file\n",
    "        item_vecs_file.close()\n",
    "        return a\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading the matrix\n",
    "def load_matrix(filepath, num_users, num_items):\n",
    "    counts = np.zeros((num_users, num_items))\n",
    "    for i, line in enumerate(open(filepath, 'r')):\n",
    "        user, item, count = line.strip().split(',')\n",
    "        user = int(user)\n",
    "        item = int(item)\n",
    "        count = float(count)\n",
    "        counts[user][item] = count\n",
    "    return counts\n",
    "\n",
    "def sparser(originalmatrix, num_users, num_items):\n",
    "    counts = sparse.dok_matrix((num_users, num_items), dtype=float)\n",
    "    for i in range(len(originalmatrix)):\n",
    "        row = originalmatrix[i]\n",
    "        for j in range(len(row)):\n",
    "            if row[j] > 0:\n",
    "                user = int(i)\n",
    "                item = int(j)\n",
    "                count = float(row[j])\n",
    "                counts[user, item] = count\n",
    "    counts = counts.tocsr()\n",
    "    return counts\n",
    "\n",
    "#Please enter the path to the input data\n",
    "filepath = 'C:/Users/peter/Documents/uvt/implicit/finalxdata.csv'\n",
    "matrix = load_matrix(filepath, num_users = 6518, num_items = 4036)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions for the hiding of the user-item interactions for the validation and test set\n",
    "def interactiontodrop(rownumber, row):\n",
    "    scoreditems = []\n",
    "    items = dict(enumerate(row))\n",
    "    for k in items:\n",
    "        if items[k] > 0:\n",
    "            scoreditems.append(k)\n",
    "    random.seed(1)\n",
    "    if not scoreditems:\n",
    "        print(rownumber)\n",
    "    return [rownumber, random.choice(scoreditems)]\n",
    "\n",
    "\n",
    "def dropper(matrix, n):\n",
    "    r = list(range(len(matrix)))\n",
    "    random.seed(1)\n",
    "    sample = random.sample(r,n)\n",
    "    toodrop = []\n",
    "    for x in sample:\n",
    "        toodrop.append(interactiontodrop(x, matrix[x]))\n",
    "    for item in toodrop:\n",
    "        matrix[item[0]][item[1]] = 0\n",
    "    return toodrop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating the test set\n",
    "test_set = dropper(matrix, 650)\n",
    "originalmatrix = matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functions for the evaluation\n",
    "\n",
    "def prediction(userid, user_vectors, item_vectors, user_biases, item_biases, originalmatrix, n=4036):\n",
    "    pred = np.dot(user_vectors[userid], item_vectors.T)\n",
    "    pred = pred + user_biases[userid]\n",
    "    pred=pred.tolist()\n",
    "    item_biases=item_biases.tolist()\n",
    "    new_item_biases = [i[0] for i in item_biases]\n",
    "    pred = [sum(x) for x in zip(pred, new_item_biases)]\n",
    "    pred = np.asarray(pred)\n",
    "    predictions = np.exp(pred)/(1+np.exp(pred))\n",
    "    for i in range(4036):\n",
    "        if originalmatrix[userid][i] > 0:\n",
    "            predictions[i] = -6000\n",
    "    dict = {key: value for (key, value) in (enumerate(predictions))}\n",
    "    sorted_dict = sorted(dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    recommendation = []\n",
    "    for i in range(n):\n",
    "        recommendation.append(sorted_dict[i][0])\n",
    "    return recommendation\n",
    "\n",
    "\n",
    "def hlu_calc(dropped_items, coefficients, originalmatrix):\n",
    "    model_hlus = []\n",
    "    user_vectors, item_vectors, user_biases, item_biases = coefficients\n",
    "    for b in dropped_items:\n",
    "        ranking = (prediction(b[0], user_vectors, item_vectors, user_biases, item_biases,\n",
    "                                       originalmatrix).index(b[1]) + 1)\n",
    "        if ranking < 250:\n",
    "            ind_hlu = 1 / (2 ** ((ranking) / 10))\n",
    "            model_hlus.append(ind_hlu)\n",
    "        else:\n",
    "            model_hlus.append(0.0000000001)\n",
    "    return model_hlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model building part. Only this time, my pc needs too much memory space. However, I can run the code if I copy all notebook cells directly into a python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-13e6037236b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlambdaa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlambdaa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_factors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcoefficients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_vectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_biases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem_biases\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0fca93cdbd9a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[1;31m# take step towards gradient of deriv of log likelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[1;31m# we take a step in positive direction because we are maximizing LL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0muser_vec_deriv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_bias_deriv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderiv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0muser_vec_deriv_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_vec_deriv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0muser_bias_deriv_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_bias_deriv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0fca93cdbd9a>\u001b[0m in \u001b[0;36mderiv\u001b[0;34m(self, user)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mA\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcounts\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "alpha = 10\n",
    "matrix = matrix*alpha\n",
    "lambdaa = 50\n",
    "t1 = time.time()\n",
    "model = LogisticMF(counts=matrix, reg_param=lambdaa, num_factors=40, iterations=20, gamma = 1)\n",
    "model.train_model()\n",
    "print(\"This algorithm is not that fast, it took {} seconds to build the model\".format(time.time()-t1))\n",
    "coefficients = [model.user_vectors, model.item_vectors, model.user_biases, model.item_biases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coefficients' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fca1168264a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m print(np.average(hlu_calc(test_set, coefficients, originalmatrix\n\u001b[0m\u001b[1;32m      2\u001b[0m                          )))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'coefficients' is not defined"
     ]
    }
   ],
   "source": [
    "print(np.average(hlu_calc(test_set, coefficients, originalmatrix\n",
    "                         )))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
